
# ğŸ§  Emotion-Aware MLP on MNIST  

![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)

![License](https://img.shields.io/badge/license-MIT-green.svg)


* A Human-Inspired Emotion-Aware MLP for MNIST Classification. A human-like twist to learning with just Python & NumPy*  

---

## ğŸŒŸ Why build this from scratch?
Most AI models today rely on big libraries like PyTorch or TensorFlow. Theyâ€™re powerful, but they hide the actual learning process.  

This project rebuilds an MNIST digit classifier **from scratch using only Python and NumPy**.  
That way, we *see AIâ€™s brain at its rawest form*.  

Think of it like cooking ğŸ‘©â€ğŸ³:  
- Instant food ğŸœ â†’ Quick, but soulless.  
- Cooking from scratch ğŸ² â†’ Slower, but you *understand the ingredients* and enjoy the process.
- Finding the joy in cooking, feeling the happiness when someone preises your cooking or when your mother cooks you your favorite food with lot of love.

Thatâ€™s exactly what we did here â€” we cooked up AI from scratch.  

---

## ğŸ¤” Why another MNIST-from-scratch project?

You might be thinking:
"There are already thousands of MNIST projects in Python + NumPy. Why do we need one more?"

Hereâ€™s the twist:
Most projects stop at accuracy. They build the same old recipe â€” input â†’ hidden layers â†’ output.

But ask yourself this:

Does a student learn only by repeating?

Does a cook follow a recipe blindly?

Does a human brain treat easy and hard problems the same way?

The answer is NO.

Humans donâ€™t just compute.
We feel confusion, adjust strategies, focus harder when needed, and gain confidence.

Thatâ€™s what this project brings:
ğŸ‘‰ An Emotional Gate inside a neural network.

Itâ€™s not about just recognizing digits.
Itâ€™s about giving AI the power to feel its own uncertainty â€” and learn better because of it.

---

## ğŸ’¡ What makes this project unique?
Normal AI does this:  
**Input â†’ Hidden Layers â†’ Output**  

But humans donâ€™t just compute. We also **feel**.  
When your mom cooks, itâ€™s not just following a recipe:  
- She tastes while cooking ğŸ˜‹  
- Adjusts spices ğŸŒ¶ï¸  
- Serves with confidence and joy â¤ï¸  

That â€œextra layer of feelingâ€ is what we added â€” an **Emotional Gate**.  

---

## ğŸ”‘ Why add the Emotional Gate *after hidden layers*?
Hidden layers are like the **thinking brain** ğŸ§ , chopping and mixing ingredients (edges, curves, patterns).  

But before serving the dish (final prediction), emotions step in:  
- â€œDoes it taste right?â€  
- â€œAm I confident enough to serve it?â€  

So the **Emotional Gate** acts like a **taste test** ğŸ² before making the final decision.  

---

## âš–ï¸ With vs Without Emotional Gate  

**Without Emotional Gate (Normal Neural Net):**  
ğŸ¤– Like a robot cook:  
- Follows the recipe exactly.  
- Doesnâ€™t taste or adjust.  
- Food may be edibleâ€¦ but bland.
- The network treats every sample the same.
- Sometimes overfits on easy examples.
- Sometimes ignores hard ones. 

**With Emotional Gate (Our Model):**  
ğŸ‘©â€ğŸ³ Like a human cook:  
- Tastes while cooking.  
- Fixes mistakes (too salty â†’ balance it).  
- Serves with confidence and happiness â¤ï¸.
- Adjusts learning based on â€œconfusion.â€
- Pays more attention to tricky digits.
- Learns more human-like: balancing confidence and uncertainty.

âœ¨ Example:  
- **Without Emotion:** Model sees a â€œ3â€ but mispredicts as â€œ8â€. It just accepts the mistake.  
- **With Emotion:** Model says *â€œHmm, Iâ€™m not confident â€” let me focus harder on this tricky digit.â€* â†’ Improves accuracy.
  
---

## ğŸ­ Why call it an Emotional Gate if itâ€™s just math?

Of course, this neural network doesnâ€™t truly â€œfeelâ€ like a human. Itâ€™s still numbers, weights, and equations.
So why call it an Emotional Gate?

Because in humans, emotions are not just â€œfeelingsâ€ â€” they are signals that guide learning:

Confusion makes us slow down and try harder ğŸ˜•

Confidence makes us move faster ğŸ˜

Curiosity makes us focus more ğŸ”

We took that principle and encoded it mathematically:

If the network feels uncertain about a digit, the Emotional Gate tells it to adjust learning.

If it feels confident, it passes the decision smoothly.

So the Emotional Gate is like a mathematical metaphor for emotions:

Not real feelings, but a way to make the network behave a little more like us.


---

## ğŸ–¼ï¸ Model Flow :  
Input (Pixels)  
      â†“  
 [ Hidden Layer 1 ] â†’ detects edges  
      â†“  
 [ Hidden Layer 2 ] â†’ detects shapes  
      â†“  
 [ Hidden Layer 3 ] â†’ combines patterns â”€â”€â”€â” Thinking Brain ğŸ§   
      â†“                                    â”‚  
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚  
   â”‚   Emotional      â”‚ â†’ "Does this feel right?" ğŸ²  
   â”‚       Gate       â”‚ â†’ "Confident or Confused?" ğŸ¤”  
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  
      â†“  
 [ Output Layer ] â†’ Final Prediction (Digit 0â€“9)

 ---

 ## Setup :
 
 1) Prerequisites
 
  - Python 3.8+
  - pip

 2) Get the code :

 git clone https://github.com/<your-username>/emotion-aware-mlp-mnist.git 
 cd emotion-aware-mlp-mnist

 3) Create a Virtual Environment :

  python -m venv .venv
  #Windows
  .venv\Scripts\activate
  #macOS/Linux
  source .venv/bin/activate

 4) Install Dependencies :

    pip install -r requirements.txt

 5) Project Structure :

    emotion-aware-mlp-mnist/
â”œâ”€ data/                 # MNIST .gz (auto-downloaded)
â”œâ”€ assets/               # checkpoints/plots (optional)
â”œâ”€ emotion_mlp/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ data.py            # download & load MNIST
â”‚  â”œâ”€ utils.py           # activations, loss, metrics
â”‚  â”œâ”€ model.py           # EmotionAwareMLP (gated MLP)
â”‚  â””â”€ train.py           # training loop
â”œâ”€ scripts/
â”‚  â””â”€ run_training.py
â”œâ”€ requirements.txt
â””â”€ .gitignore

 6) Train the Model :

 #Option A: as a module
 python -m emotion_mlp.train

 #Option B: via the script
 python scripts/run_training.py

7) Configuration
- Adjust defaults inside emotion_mlp/train.py:
- epochs (default: 100)
- lr (learning rate, default: 0.01)
- batch_size (default: 64)
- patience (early stopping, default: 8)
- L2 coefficient in model constructor (default: 0.001)

---

## Results

- Early stopping at epoch 72.
- Best metrics:
  - Train Acc: 99.83% (epoch 71)
  - Test/Val Acc: 98.06% (epoch 71)
  - Loss: 0.0137 (epoch 71)

### Training curve (highlights)
- 90%+ accuracy within 3 epochs.
- 96â€“97% plateau by ~epoch 15.
- Peak around 98.1% test accuracy near epoch 71.
- Early stopping prevented overfitting after later fluctuations.

> The â€œemotional gateâ€ (sigmoid modulation of the last hidden layer) provides inputâ€‘dependent feature weighting, which helped push accuracy beyond the baseline plateau while maintaining generalization.

This project is not about achieving the highest accuracy or building a new model from scratch.
Instead, the main objective is:

To show how an emotion-inspired mechanism (Emotional Gate) can be integrated into a machine learning model.

To demonstrate that even in simple tasks (like MNIST digit classification), an emotion-like layer can improve decision-making.

To provide evidence that human-inspired ideas (like confusion, hesitation, and confidence) can make artificial intelligence more adaptive.


  

    


  


